{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Encoder Reranking in RAG Systems with Gemini\n",
    "\n",
    "This notebook demonstrates advanced cross-encoder reranking in Retrieval-Augmented Generation (RAG) systems using:\n",
    "- **Cross-Encoder Models** for sophisticated query-document relevance scoring\n",
    "- **Sentence Transformers** for initial retrieval\n",
    "- **FAISS** for efficient vector search\n",
    "- **Google Gemini** for intelligent explanations and final answer generation\n",
    "\n",
    "## Cross-Encoder vs Bi-Encoder (Semantic) Reranking\n",
    "\n",
    "**Bi-Encoder (Semantic Reranking):**\n",
    "- Encodes query and documents separately\n",
    "- Computes similarity using vector operations\n",
    "- Fast but limited interaction between query and document\n",
    "\n",
    "**Cross-Encoder Reranking:**\n",
    "- Processes query-document pairs jointly\n",
    "- Deep attention mechanisms between query and document tokens\n",
    "- More accurate but computationally expensive\n",
    "- Better understanding of relevance and context\n",
    "\n",
    "## Project Overview\n",
    "We'll build an advanced question-answering system about space exploration that:\n",
    "1. Retrieves relevant documents using bi-encoder embeddings\n",
    "2. Reranks results using cross-encoder models for superior relevance\n",
    "3. Uses Gemini to provide comprehensive answers with detailed explanations\n",
    "4. Compares cross-encoder vs bi-encoder performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers faiss-cpu numpy pandas google-generativeai python-dotenv scikit-learn transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import time\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import google.generativeai as genai\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "# Get your API key from https://makersuite.google.com/app/apikey\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY') or 'your-gemini-api-key-here'\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Initialize Gemini model\n",
    "gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "print(\"‚úÖ Gemini API configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Dataset: Space Exploration Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended knowledge base about space exploration for better cross-encoder demonstration\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Mars Exploration and Life Detection\",\n",
    "        \"content\": \"Mars exploration has been a key focus of space agencies worldwide, particularly in the search for signs of past or present life. NASA's Mars rovers, including Perseverance and Curiosity, have provided invaluable data about the Red Planet's geology, climate, and potential for past life. Perseverance carries sophisticated instruments like MOXIE for oxygen production and the SUPERCAM laser spectrometer. The planet's thin atmosphere, composed mainly of carbon dioxide, presents unique challenges for exploration missions. Recent discoveries of seasonal methane emissions and ancient riverbeds suggest Mars may have once harbored microbial life.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"International Space Station Scientific Research\",\n",
    "        \"content\": \"The International Space Station (ISS) serves as a premier microgravity laboratory where astronauts conduct cutting-edge scientific experiments across multiple disciplines. Located approximately 408 kilometers above Earth, the ISS completes an orbit around our planet every 90 minutes, providing a unique platform for research. The station supports investigations in biology, physics, astronomy, materials science, and human physiology. Experiments include studying protein crystallization, plant growth in microgravity, fluid physics, and the effects of long-duration spaceflight on the human body. The ISS has been continuously inhabited since November 2000, representing unprecedented international cooperation in space exploration.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"Apollo Program and Lunar Exploration Legacy\",\n",
    "        \"content\": \"The Apollo program represents humanity's greatest achievement in space exploration, successfully landing twelve astronauts on the Moon between 1969 and 1972. Six successful Moon landings were completed, with Apollo 11 being the historic first on July 20, 1969. Neil Armstrong and Buzz Aldrin were the first humans to walk on the lunar surface, while Michael Collins orbited above in the command module. The program involved over 400,000 people and cost $25 billion (1970s dollars). Apollo missions collected 842 pounds of lunar samples, conducted extensive geological surveys, and proved that large-scale human space exploration was possible. The legacy continues with NASA's Artemis program aiming to return humans to the Moon by 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Exoplanet Discovery and Characterization Methods\",\n",
    "        \"content\": \"Astronomers employ sophisticated methods to discover and characterize exoplanets, with the transit method and radial velocity method being the most successful. The Kepler Space Telescope revolutionized exoplanet science by discovering over 2,600 confirmed planets, while TESS (Transiting Exoplanet Survey Satellite) continues this work. The transit method detects the slight dimming when a planet passes in front of its star, while radial velocity measures the gravitational wobble of stars caused by orbiting planets. Many exoplanets orbit in their star's habitable zone, where liquid water could potentially exist. Advanced techniques like direct imaging and gravitational microlensing are revealing diverse planetary systems, including super-Earths, hot Jupiters, and potentially habitable rocky worlds.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"SpaceX Innovation and Reusable Rocket Technology\",\n",
    "        \"content\": \"SpaceX has fundamentally transformed the space industry through breakthrough reusable rocket technology and ambitious mission goals. The Falcon 9 rocket features a first stage that can autonomously land back on Earth, either on drone ships or landing pads, dramatically reducing launch costs from tens of thousands of dollars per kilogram to under $3,000. The company's Dragon spacecraft regularly transports crew and cargo to the ISS, ending America's dependence on Russian Soyuz vehicles. SpaceX's Starship project represents the next leap forward, designed as a fully reusable super heavy-lift vehicle capable of carrying 100+ tons to low Earth orbit. The ultimate goal is enabling human missions to Mars and establishing a self-sustaining colony on the Red Planet, with Elon Musk targeting the 2030s for crewed Mars missions.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"Jupiter's Moons and Astrobiology Potential\",\n",
    "        \"content\": \"Jupiter's four largest moons - Io, Europa, Ganymede, and Callisto - represent some of the most fascinating targets for astrobiology research in our solar system. Europa is particularly compelling due to its subsurface ocean beneath an icy crust, containing more water than all of Earth's oceans combined. This moon shows evidence of hydrothermal activity on its ocean floor, similar to environments on Earth where life thrives around deep-sea vents. Ganymede, the largest moon in the solar system, also harbors a subsurface ocean and has its own magnetic field. NASA's Europa Clipper mission, launching in 2024, will conduct detailed reconnaissance of Europa's ice shell and subsurface ocean. The European Space Agency's JUICE mission will study Jupiter's icy moons, focusing on Ganymede's potential habitability.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"title\": \"Solar System Formation and Planetary Science\",\n",
    "        \"content\": \"The solar system formed approximately 4.6 billion years ago from the gravitational collapse of a giant molecular cloud called the solar nebula. This process concentrated most mass at the center, forming the proto-Sun, while the remaining material formed a rotating protoplanetary disk. Through accretion, dust grains stuck together to form planetesimals, which eventually grew into planets. The inner rocky planets (Mercury, Venus, Earth, Mars) formed in the hot inner region where only refractory materials could condense, while the outer gas giants (Jupiter, Saturn, Uranus, Neptune) formed beyond the snow line where volatile compounds could freeze. Jupiter's early formation significantly influenced the architecture of the entire solar system, preventing Mars from growing larger and scattering asteroids throughout the system.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"title\": \"Space Telescopes and Deep Universe Observations\",\n",
    "        \"content\": \"Space telescopes represent humanity's most powerful tools for understanding the cosmos, operating beyond Earth's atmospheric interference to capture unprecedented views of the universe. The Hubble Space Telescope, operational since 1990, has revolutionized astronomy with discoveries including the accelerating expansion of the universe, the age of the cosmos (13.8 billion years), and detailed images of distant galaxies. The James Webb Space Telescope, Hubble's successor, observes in infrared wavelengths and can peer back to the universe's first galaxies formed just 400 million years after the Big Bang. Other specialized missions include the Spitzer Space Telescope (infrared), Chandra X-ray Observatory (X-rays), and the upcoming Nancy Grace Roman Space Telescope (wide-field surveys). These instruments have revealed exoplanets, black holes, dark matter structures, and the cosmic web.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"title\": \"Asteroid Mining and Space Resources\",\n",
    "        \"content\": \"Asteroid mining represents the next frontier in space exploration and resource utilization, with the potential to provide virtually unlimited raw materials for space-based industries and Earth's growing population. Near-Earth asteroids contain vast quantities of precious metals, rare earth elements, and water ice. A single metallic asteroid could contain more platinum than has ever been mined on Earth. Water extracted from asteroids can be split into hydrogen and oxygen for rocket fuel, enabling sustainable space exploration. Companies like Planetary Resources and Deep Space Industries are developing the technology for robotic asteroid prospecting and mining operations. NASA's OSIRIS-REx mission successfully collected samples from asteroid Bennu, demonstrating the feasibility of asteroid resource extraction. The economic potential is enormous, with some estimates suggesting asteroid mining could create the world's first trillionaire.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"title\": \"Human Spaceflight and Mars Colonization\",\n",
    "        \"content\": \"Human spaceflight to Mars represents the ultimate challenge in space exploration, requiring solutions to numerous technical, biological, and psychological obstacles. The journey to Mars takes 6-9 months each way, exposing astronauts to dangerous levels of cosmic radiation and prolonged microgravity effects including bone loss, muscle atrophy, and cardiovascular deconditioning. Psychological challenges include isolation, confinement, and delayed communication with Earth. Establishing a sustainable Mars colony requires local resource utilization (ISRU) for producing water, oxygen, and fuel from the Martian atmosphere and subsurface ice. NASA's Artemis program serves as a stepping stone, testing technologies and procedures on the Moon that will be essential for Mars missions. Private companies like SpaceX are developing the heavy-lift capabilities needed for Mars transportation, while space agencies worldwide collaborate on the technical challenges of keeping humans alive and productive on another planet.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Created enhanced knowledge base with {len(documents)} documents\")\n",
    "print(\"Sample document:\")\n",
    "print(f\"Title: {documents[0]['title']}\")\n",
    "print(f\"Content: {documents[0]['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Encoder Reranking System Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderReranker:\n",
    "    def __init__(self, \n",
    "                 bi_encoder_model: str = 'all-MiniLM-L6-v2',\n",
    "                 cross_encoder_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the cross-encoder reranker with both bi-encoder and cross-encoder models.\n",
    "        \n",
    "        Args:\n",
    "            bi_encoder_model: Name of the bi-encoder model for initial retrieval\n",
    "            cross_encoder_model: Name of the cross-encoder model for reranking\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Loading bi-encoder model: {bi_encoder_model}\")\n",
    "        self.bi_encoder = SentenceTransformer(bi_encoder_model)\n",
    "        \n",
    "        print(f\"üîÑ Loading cross-encoder model: {cross_encoder_model}\")\n",
    "        self.cross_encoder = CrossEncoder(cross_encoder_model)\n",
    "        \n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "        self.documents = None\n",
    "        \n",
    "    def build_index(self, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        Build FAISS index from documents using bi-encoder.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document dictionaries\n",
    "        \"\"\"\n",
    "        print(\"üîß Building document embeddings and FAISS index...\")\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Combine title and content for better semantic representation\n",
    "        texts = [f\"{doc['title']}. {doc['content']}\" for doc in documents]\n",
    "        \n",
    "        # Generate embeddings using bi-encoder\n",
    "        self.embeddings = self.bi_encoder.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        normalized_embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "        self.index.add(normalized_embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"‚úÖ Index built with {len(documents)} documents (dimension: {dimension})\")\n",
    "    \n",
    "    def initial_retrieval(self, query: str, k: int = 20) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        Perform initial retrieval using bi-encoder and FAISS.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of documents to retrieve (higher for cross-encoder reranking)\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, score) tuples\n",
    "        \"\"\"\n",
    "        # Encode query using bi-encoder\n",
    "        query_embedding = self.bi_encoder.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Return documents with scores\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx != -1:  # Valid index\n",
    "                results.append((self.documents[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cross_encoder_rerank(self, query: str, retrieved_docs: List[Tuple[Dict, float]], \n",
    "                            top_k: int = 5) -> List[Tuple[Dict, float, float]]:\n",
    "        \"\"\"\n",
    "        Rerank retrieved documents using cross-encoder model.\n",
    "        \n",
    "        Args:\n",
    "            query: Original search query\n",
    "            retrieved_docs: List of (document, initial_score) tuples\n",
    "            top_k: Number of top documents to return after reranking\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, initial_score, cross_encoder_score) tuples\n",
    "        \"\"\"\n",
    "        if not retrieved_docs:\n",
    "            return []\n",
    "        \n",
    "        print(f\"üîÑ Cross-encoder reranking {len(retrieved_docs)} documents...\")\n",
    "        \n",
    "        # Prepare query-document pairs for cross-encoder\n",
    "        query_doc_pairs = []\n",
    "        for doc, _ in retrieved_docs:\n",
    "            # Use both title and content for cross-encoder evaluation\n",
    "            doc_text = f\"{doc['title']}. {doc['content']}\"\n",
    "            query_doc_pairs.append([query, doc_text])\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        cross_encoder_scores = self.cross_encoder.predict(query_doc_pairs)\n",
    "        \n",
    "        # Combine with original documents and scores\n",
    "        reranked_results = []\n",
    "        for i, (doc, initial_score) in enumerate(retrieved_docs):\n",
    "            cross_score = float(cross_encoder_scores[i])\n",
    "            reranked_results.append((doc, initial_score, cross_score))\n",
    "        \n",
    "        # Sort by cross-encoder score (descending)\n",
    "        reranked_results.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        print(f\"‚úÖ Reranking complete, returning top {top_k} documents\")\n",
    "        return reranked_results[:top_k]\n",
    "    \n",
    "    def bi_encoder_rerank(self, query: str, retrieved_docs: List[Tuple[Dict, float]], \n",
    "                         top_k: int = 5) -> List[Tuple[Dict, float, float]]:\n",
    "        \"\"\"\n",
    "        Rerank using bi-encoder for comparison purposes.\n",
    "        \n",
    "        Args:\n",
    "            query: Original search query\n",
    "            retrieved_docs: List of (document, initial_score) tuples\n",
    "            top_k: Number of top documents to return after reranking\n",
    "            \n",
    "        Returns:\n",
    "            List of (document, initial_score, bi_encoder_score) tuples\n",
    "        \"\"\"\n",
    "        if not retrieved_docs:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.bi_encoder.encode([query])\n",
    "        \n",
    "        # Encode retrieved documents\n",
    "        doc_texts = [f\"{doc['title']}. {doc['content']}\" for doc, _ in retrieved_docs]\n",
    "        doc_embeddings = self.bi_encoder.encode(doc_texts)\n",
    "        \n",
    "        # Compute semantic similarity scores\n",
    "        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "        \n",
    "        # Combine with original documents and scores\n",
    "        reranked_results = []\n",
    "        for i, (doc, initial_score) in enumerate(retrieved_docs):\n",
    "            bi_encoder_score = similarities[i]\n",
    "            reranked_results.append((doc, initial_score, bi_encoder_score))\n",
    "        \n",
    "        # Sort by bi-encoder score (descending)\n",
    "        reranked_results.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return reranked_results[:top_k]\n",
    "\n",
    "print(\"‚úÖ CrossEncoderReranker class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG System with Cross-Encoder Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGSystem:\n",
    "    def __init__(self, reranker: CrossEncoderReranker, gemini_model):\n",
    "        self.reranker = reranker\n",
    "        self.gemini_model = gemini_model\n",
    "    \n",
    "    def generate_comprehensive_answer(self, query: str, context_docs: List[Dict], \n",
    "                                    ranking_method: str = \"cross-encoder\") -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using Gemini with retrieved context and ranking analysis.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context_docs: List of relevant documents\n",
    "            ranking_method: Method used for ranking (for explanation)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing answer and analysis\n",
    "        \"\"\"\n",
    "        # Prepare context\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}: {doc['title']}\\n{doc['content']}\"\n",
    "            for i, doc in enumerate(context_docs)\n",
    "        ])\n",
    "        \n",
    "        # Create enhanced prompt for Gemini\n",
    "        prompt = f\"\"\"\n",
    "You are an expert space exploration assistant with access to a carefully curated knowledge base. The documents provided have been ranked using {ranking_method} reranking to ensure maximum relevance to the user's question.\n",
    "\n",
    "Context Documents (ranked by relevance):\n",
    "{context_text}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Please provide a comprehensive response with:\n",
    "\n",
    "1. **Direct Answer:** A clear, concise answer to the user's specific question\n",
    "\n",
    "2. **Detailed Explanation:** Expand on your answer with relevant details from the provided documents\n",
    "\n",
    "3. **Source Analysis:** Explain which documents were most valuable for answering this question and why the {ranking_method} method likely ranked them highly\n",
    "\n",
    "4. **Cross-References:** Identify connections between different documents that support or enhance your answer\n",
    "\n",
    "5. **Knowledge Gaps:** Note any aspects of the question that aren't fully covered by the available documents\n",
    "\n",
    "6. **Additional Context:** Provide any relevant background information that helps understand the broader implications of your answer\n",
    "\n",
    "Format your response clearly with the section headers above.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.gemini_model.generate_content(prompt)\n",
    "            return {\n",
    "                'answer': response.text,\n",
    "                'context_docs': context_docs,\n",
    "                'query': query,\n",
    "                'ranking_method': ranking_method\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'answer': f\"Error generating response: {str(e)}\",\n",
    "                'context_docs': context_docs,\n",
    "                'query': query,\n",
    "                'ranking_method': ranking_method\n",
    "            }\n",
    "    \n",
    "    def process_query_with_comparison(self, query: str, retrieve_k: int = 15, rerank_k: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline with both cross-encoder and bi-encoder comparison.\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            retrieve_k: Number of documents to initially retrieve\n",
    "            rerank_k: Number of documents to keep after reranking\n",
    "            \n",
    "        Returns:\n",
    "            Complete response with cross-encoder results and comparison\n",
    "        \"\"\"\n",
    "        print(f\"üîç Processing query: '{query}'\")\n",
    "        \n",
    "        # Step 1: Initial retrieval\n",
    "        print(f\"üì• Retrieving top {retrieve_k} documents...\")\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = self.reranker.initial_retrieval(query, k=retrieve_k)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Step 2: Cross-encoder reranking\n",
    "        print(f\"üéØ Cross-encoder reranking to top {rerank_k} documents...\")\n",
    "        start_time = time.time()\n",
    "        cross_encoder_results = self.reranker.cross_encoder_rerank(query, retrieved_docs, top_k=rerank_k)\n",
    "        cross_encoder_time = time.time() - start_time\n",
    "        \n",
    "        # Step 3: Bi-encoder reranking for comparison\n",
    "        print(f\"üìä Bi-encoder reranking for comparison...\")\n",
    "        start_time = time.time()\n",
    "        bi_encoder_results = self.reranker.bi_encoder_rerank(query, retrieved_docs, top_k=rerank_k)\n",
    "        bi_encoder_time = time.time() - start_time\n",
    "        \n",
    "        # Extract documents for context (using cross-encoder results)\n",
    "        context_docs = [doc for doc, _, _ in cross_encoder_results]\n",
    "        \n",
    "        # Step 4: Generate answer with Gemini\n",
    "        print(\"ü§ñ Generating comprehensive answer with Gemini...\")\n",
    "        start_time = time.time()\n",
    "        result = self.generate_comprehensive_answer(query, context_docs, \"cross-encoder\")\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        # Add comprehensive analysis\n",
    "        result['analysis'] = {\n",
    "            'initial_retrieval_count': len(retrieved_docs),\n",
    "            'cross_encoder_results': [{\n",
    "                'title': doc['title'],\n",
    "                'initial_score': initial_score,\n",
    "                'cross_encoder_score': cross_score,\n",
    "                'rank': i + 1\n",
    "            } for i, (doc, initial_score, cross_score) in enumerate(cross_encoder_results)],\n",
    "            'bi_encoder_results': [{\n",
    "                'title': doc['title'],\n",
    "                'initial_score': initial_score,\n",
    "                'bi_encoder_score': bi_score,\n",
    "                'rank': i + 1\n",
    "            } for i, (doc, initial_score, bi_score) in enumerate(bi_encoder_results)],\n",
    "            'timing': {\n",
    "                'retrieval_time': retrieval_time,\n",
    "                'cross_encoder_time': cross_encoder_time,\n",
    "                'bi_encoder_time': bi_encoder_time,\n",
    "                'generation_time': generation_time,\n",
    "                'total_time': retrieval_time + cross_encoder_time + generation_time\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Query processing complete!\")\n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ AdvancedRAGSystem class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize and Build the Advanced System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cross-encoder reranker\n",
    "print(\"üöÄ Initializing Cross-Encoder Reranking System...\")\n",
    "cross_encoder_reranker = CrossEncoderReranker(\n",
    "    bi_encoder_model='all-MiniLM-L6-v2',\n",
    "    cross_encoder_model='cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    ")\n",
    "\n",
    "# Build the index with our enhanced documents\n",
    "cross_encoder_reranker.build_index(documents)\n",
    "\n",
    "# Initialize the complete advanced RAG system\n",
    "advanced_rag_system = AdvancedRAGSystem(cross_encoder_reranker, gemini_model)\n",
    "\n",
    "print(\"üöÄ Advanced RAG System with Cross-Encoder Reranking is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demo: Cross-Encoder vs Bi-Encoder Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries specifically designed to show cross-encoder advantages\n",
    "test_queries = [\n",
    "    \"How do space agencies search for signs of life beyond Earth?\",\n",
    "    \"What are the main challenges of establishing a human presence on Mars?\",\n",
    "    \"Compare the scientific value of space telescopes versus robotic missions to other planets\"\n",
    "]\n",
    "\n",
    "def display_comprehensive_results(result: Dict):\n",
    "    \"\"\"\n",
    "    Display results with detailed analysis and comparison.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"üîç QUERY: {result['query']}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Show timing information\n",
    "    timing = result['analysis']['timing']\n",
    "    print(f\"\\n‚è±Ô∏è PERFORMANCE METRICS:\")\n",
    "    print(f\"   Retrieval Time:     {timing['retrieval_time']:.3f}s\")\n",
    "    print(f\"   Cross-Encoder Time: {timing['cross_encoder_time']:.3f}s\")\n",
    "    print(f\"   Bi-Encoder Time:    {timing['bi_encoder_time']:.3f}s\")\n",
    "    print(f\"   Generation Time:    {timing['generation_time']:.3f}s\")\n",
    "    print(f\"   Total Time:         {timing['total_time']:.3f}s\")\n",
    "    \n",
    "    # Show ranking comparison\n",
    "    print(f\"\\nüìä RANKING COMPARISON (Top {len(result['analysis']['cross_encoder_results'])}):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    cross_results = result['analysis']['cross_encoder_results']\n",
    "    bi_results = result['analysis']['bi_encoder_results']\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(f\"{'Rank':<4} {'Cross-Encoder Results':<35} {'Bi-Encoder Results':<35}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(len(cross_results)):\n",
    "        cross_title = cross_results[i]['title'][:30] + \"...\" if len(cross_results[i]['title']) > 30 else cross_results[i]['title']\n",
    "        bi_title = bi_results[i]['title'][:30] + \"...\" if len(bi_results[i]['title']) > 30 else bi_results[i]['title']\n",
    "        \n",
    "        print(f\"{i+1:<4} {cross_title:<35} {bi_title:<35}\")\n",
    "        print(f\"     Score: {cross_results[i]['cross_encoder_score']:.4f}             Score: {bi_results[i]['bi_encoder_score']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Analyze ranking differences\n",
    "    cross_titles = [r['title'] for r in cross_results]\n",
    "    bi_titles = [r['title'] for r in bi_results]\n",
    "    \n",
    "    if cross_titles != bi_titles:\n",
    "        print(\"‚ú® RANKING DIFFERENCES DETECTED!\")\n",
    "        print(\"Cross-encoder provided different ranking than bi-encoder.\")\n",
    "    else:\n",
    "        print(\"‚û°Ô∏è Both methods produced the same ranking.\")\n",
    "    \n",
    "    # Show Gemini's response\n",
    "    print(f\"\\nü§ñ GEMINI RESPONSE ({result['ranking_method'].upper()}):\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['answer'])\n",
    "    print(\"\\n\" + \"=\" * 90 + \"\\n\")\n",
    "\n",
    "# Run demo with first query\n",
    "demo_query = test_queries[0]\n",
    "result = advanced_rag_system.process_query_with_comparison(demo_query, retrieve_k=12, rerank_k=4)\n",
    "display_comprehensive_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Cross-Encoder vs Bi-Encoder Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ranking_differences(query: str, k: int = 8):\n",
    "    \"\"\"\n",
    "    Detailed analysis of how cross-encoder and bi-encoder rankings differ.\n",
    "    \"\"\"\n",
    "    print(f\"üî¨ DETAILED RANKING ANALYSIS\")\n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    \n",
    "    # Get initial retrieval results\n",
    "    initial_results = cross_encoder_reranker.initial_retrieval(query, k=k*2)\n",
    "    \n",
    "    # Get both reranking results\n",
    "    cross_results = cross_encoder_reranker.cross_encoder_rerank(query, initial_results, top_k=k)\n",
    "    bi_results = cross_encoder_reranker.bi_encoder_rerank(query, initial_results, top_k=k)\n",
    "    \n",
    "    # Create detailed comparison\n",
    "    print(\"üìã DOCUMENT-BY-DOCUMENT COMPARISON:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(k):\n",
    "        cross_doc, cross_initial, cross_score = cross_results[i]\n",
    "        bi_doc, bi_initial, bi_score = bi_results[i]\n",
    "        \n",
    "        print(f\"\\nüìÑ RANK {i+1}:\")\n",
    "        print(f\"Cross-Encoder: {cross_doc['title']}\")\n",
    "        print(f\"   Score: {cross_score:.4f} (Initial: {cross_initial:.4f})\")\n",
    "        \n",
    "        print(f\"Bi-Encoder:    {bi_doc['title']}\")\n",
    "        print(f\"   Score: {bi_score:.4f} (Initial: {bi_initial:.4f})\")\n",
    "        \n",
    "        if cross_doc['id'] == bi_doc['id']:\n",
    "            print(\"   ‚úÖ Same document selected by both methods\")\n",
    "        else:\n",
    "            print(\"   üîÑ Different documents selected\")\n",
    "    \n",
    "    # Calculate ranking similarity metrics\n",
    "    cross_ids = [doc['id'] for doc, _, _ in cross_results]\n",
    "    bi_ids = [doc['id'] for doc, _, _ in bi_results]\n",
    "    \n",
    "    # Overlap at different positions\n",
    "    overlap_metrics = {}\n",
    "    for pos in [1, 3, 5]:\n",
    "        if pos <= len(cross_ids):\n",
    "            cross_top = set(cross_ids[:pos])\n",
    "            bi_top = set(bi_ids[:pos])\n",
    "            overlap = len(cross_top.intersection(bi_top)) / pos\n",
    "            overlap_metrics[f'top_{pos}'] = overlap\n",
    "    \n",
    "    print(f\"\\nüìä RANKING SIMILARITY METRICS:\")\n",
    "    for metric, value in overlap_metrics.items():\n",
    "        print(f\"   {metric.replace('_', ' ').title()} Overlap: {value:.2%}\")\n",
    "    \n",
    "    # Score distribution analysis\n",
    "    cross_scores = [score for _, _, score in cross_results]\n",
    "    bi_scores = [score for _, _, score in bi_results]\n",
    "    \n",
    "    print(f\"\\nüìà SCORE DISTRIBUTION:\")\n",
    "    print(f\"   Cross-Encoder: Mean={np.mean(cross_scores):.4f}, Std={np.std(cross_scores):.4f}\")\n",
    "    print(f\"   Bi-Encoder:    Mean={np.mean(bi_scores):.4f}, Std={np.std(bi_scores):.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'cross_results': cross_results,\n",
    "        'bi_results': bi_results,\n",
    "        'overlap_metrics': overlap_metrics\n",
    "    }\n",
    "\n",
    "# Analyze ranking differences for different query types\n",
    "for query in test_queries[:2]:\n",
    "    analysis = analyze_ranking_differences(query)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance and Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_performance_analysis(queries: List[str]):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of cross-encoder vs bi-encoder performance.\n",
    "    \"\"\"\n",
    "    print(\"üî¨ COMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {\n",
    "        'queries': queries,\n",
    "        'cross_encoder_times': [],\n",
    "        'bi_encoder_times': [],\n",
    "        'retrieval_times': [],\n",
    "        'overlap_scores': [],\n",
    "        'score_differences': []\n",
    "    }\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"\\nProcessing query {i+1}/{len(queries)}: '{query[:60]}...'\")\n",
    "        \n",
    "        # Measure retrieval time\n",
    "        start_time = time.time()\n",
    "        retrieved_docs = cross_encoder_reranker.initial_retrieval(query, k=15)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        results['retrieval_times'].append(retrieval_time)\n",
    "        \n",
    "        # Measure cross-encoder time\n",
    "        start_time = time.time()\n",
    "        cross_results = cross_encoder_reranker.cross_encoder_rerank(query, retrieved_docs, top_k=5)\n",
    "        cross_encoder_time = time.time() - start_time\n",
    "        results['cross_encoder_times'].append(cross_encoder_time)\n",
    "        \n",
    "        # Measure bi-encoder time\n",
    "        start_time = time.time()\n",
    "        bi_results = cross_encoder_reranker.bi_encoder_rerank(query, retrieved_docs, top_k=5)\n",
    "        bi_encoder_time = time.time() - start_time\n",
    "        results['bi_encoder_times'].append(bi_encoder_time)\n",
    "        \n",
    "        # Calculate overlap\n",
    "        cross_ids = set([doc['id'] for doc, _, _ in cross_results])\n",
    "        bi_ids = set([doc['id'] for doc, _, _ in bi_results])\n",
    "        overlap = len(cross_ids.intersection(bi_ids)) / len(cross_ids)\n",
    "        results['overlap_scores'].append(overlap)\n",
    "        \n",
    "        # Score differences\n",
    "        cross_scores = [score for _, _, score in cross_results]\n",
    "        bi_scores = [score for _, _, score in bi_results]\n",
    "        score_diff = np.mean(cross_scores) - np.mean(bi_scores)\n",
    "        results['score_differences'].append(score_diff)\n",
    "        \n",
    "        print(f\"  Retrieval: {retrieval_time:.3f}s | Cross-Encoder: {cross_encoder_time:.3f}s | Bi-Encoder: {bi_encoder_time:.3f}s\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Queries processed: {len(queries)}\")\n",
    "    print(f\"\\n‚è±Ô∏è TIMING ANALYSIS:\")\n",
    "    print(f\"   Avg Retrieval Time:     {np.mean(results['retrieval_times']):.3f}s\")\n",
    "    print(f\"   Avg Cross-Encoder Time: {np.mean(results['cross_encoder_times']):.3f}s\")\n",
    "    print(f\"   Avg Bi-Encoder Time:    {np.mean(results['bi_encoder_times']):.3f}s\")\n",
    "    print(f\"   Cross-Encoder Overhead: {np.mean(results['cross_encoder_times']) / np.mean(results['bi_encoder_times']):.1f}x\")\n",
    "    \n",
    "    print(f\"\\nüéØ QUALITY ANALYSIS:\")\n",
    "    print(f\"   Avg Ranking Overlap:    {np.mean(results['overlap_scores']):.2%}\")\n",
    "    print(f\"   Avg Score Difference:   {np.mean(results['score_differences']):.4f}\")\n",
    "    print(f\"   Score Std Deviation:    {np.std(results['score_differences']):.4f}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    avg_overlap = np.mean(results['overlap_scores'])\n",
    "    avg_overhead = np.mean(results['cross_encoder_times']) / np.mean(results['bi_encoder_times'])\n",
    "    \n",
    "    if avg_overlap < 0.7:\n",
    "        print(f\"   ‚úÖ Cross-encoder shows significant ranking improvements ({(1-avg_overlap):.1%} different)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è High overlap ({avg_overlap:.1%}) - consider if cross-encoder overhead is justified\")\n",
    "    \n",
    "    if avg_overhead > 10:\n",
    "        print(f\"   ‚ö†Ô∏è High computational overhead ({avg_overhead:.1f}x) - consider for high-value queries only\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Reasonable computational overhead ({avg_overhead:.1f}x)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Extended test queries for comprehensive analysis\n",
    "extended_queries = [\n",
    "    \"How do space agencies search for signs of life beyond Earth?\",\n",
    "    \"What are the main challenges of establishing a human presence on Mars?\",\n",
    "    \"Compare the scientific value of space telescopes versus robotic missions\",\n",
    "    \"What role does the International Space Station play in preparing for Mars missions?\",\n",
    "    \"How has SpaceX changed the economics of space exploration?\"\n",
    "]\n",
    "\n",
    "# Run comprehensive analysis\n",
    "performance_results = comprehensive_performance_analysis(extended_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to compare cross-encoder vs bi-encoder performance\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Cross-Encoder vs Bi-Encoder Reranking Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Timing Comparison\n",
    "ax1 = axes[0, 0]\n",
    "methods = ['Retrieval', 'Bi-Encoder', 'Cross-Encoder']\n",
    "times = [\n",
    "    np.mean(performance_results['retrieval_times']),\n",
    "    np.mean(performance_results['bi_encoder_times']),\n",
    "    np.mean(performance_results['cross_encoder_times'])\n",
    "]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(methods, times, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title('Average Processing Time by Method')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{time:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "# 2. Ranking Overlap Distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(performance_results['overlap_scores'], bins=10, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Ranking Overlap (0-1)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Ranking Overlap Scores')\n",
    "ax2.axvline(np.mean(performance_results['overlap_scores']), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(performance_results[\"overlap_scores\"]):.2f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Score Differences\n",
    "ax3 = axes[1, 0]\n",
    "query_nums = range(1, len(performance_results['score_differences']) + 1)\n",
    "ax3.plot(query_nums, performance_results['score_differences'], 'o-', color='#f39c12', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Query Number')\n",
    "ax3.set_ylabel('Score Difference (Cross - Bi)')\n",
    "ax3.set_title('Score Differences Across Queries')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "# 4. Efficiency vs Quality Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "efficiency = [1/t for t in performance_results['cross_encoder_times']]  # Inverse of time = efficiency\n",
    "quality = [(1 - overlap) for overlap in performance_results['overlap_scores']]  # 1 - overlap = uniqueness\n",
    "\n",
    "scatter = ax4.scatter(efficiency, quality, c=performance_results['score_differences'], \n",
    "                     cmap='viridis', s=100, alpha=0.7)\n",
    "ax4.set_xlabel('Efficiency (1/time)')\n",
    "ax4.set_ylabel('Ranking Uniqueness (1 - overlap)')\n",
    "ax4.set_title('Efficiency vs Quality Trade-off')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax4, label='Score Difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visualization complete! Key insights:\")\n",
    "print(f\"   ‚Ä¢ Cross-encoder is {np.mean(performance_results['cross_encoder_times'])/np.mean(performance_results['bi_encoder_times']):.1f}x slower but provides different rankings\")\n",
    "print(f\"   ‚Ä¢ Average ranking overlap: {np.mean(performance_results['overlap_scores']):.1%}\")\n",
    "print(f\"   ‚Ä¢ Cross-encoder scores are typically {'higher' if np.mean(performance_results['score_differences']) > 0 else 'lower'} than bi-encoder scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Cross-Encoder Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_cross_encoder_demo():\n",
    "    \"\"\"\n",
    "    Interactive interface for testing cross-encoder reranking.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Interactive Cross-Encoder RAG System\")\n",
    "    print(\"Ask questions about space exploration and see cross-encoder reranking in action!\")\n",
    "    print(\"\\nFeatures:\")\n",
    "    print(\"  ‚Ä¢ Real-time cross-encoder vs bi-encoder comparison\")\n",
    "    print(\"  ‚Ä¢ Detailed scoring and timing analysis\")\n",
    "    print(\"  ‚Ä¢ Comprehensive answers from Gemini\")\n",
    "    print(\"\\nType 'quit' to exit, 'help' for commands\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"üîç Your question: \").strip()\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Thanks for exploring cross-encoder reranking!\")\n",
    "                break\n",
    "            \n",
    "            if query.lower() == 'help':\n",
    "                print(\"\\nüìñ Available commands:\")\n",
    "                print(\"  ‚Ä¢ Just type your question to get a full analysis\")\n",
    "                print(\"  ‚Ä¢ 'quit' or 'exit' to stop\")\n",
    "                print(\"  ‚Ä¢ 'help' to see this message\\n\")\n",
    "                continue\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "            \n",
    "            # Process the query with full comparison\n",
    "            result = advanced_rag_system.process_query_with_comparison(query, retrieve_k=12, rerank_k=4)\n",
    "            display_comprehensive_results(result)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Thanks for exploring cross-encoder reranking!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "# Uncomment the next line to run the interactive interface\n",
    "# interactive_cross_encoder_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Demonstration: Complex Multi-Aspect Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive demonstration with a complex query\n",
    "complex_query = \"\"\"Analyze the technological and scientific progression from Apollo missions to current Mars \n",
    "exploration efforts, highlighting how lessons learned from lunar exploration inform current strategies \n",
    "for finding life on Mars and establishing sustainable human presence on other worlds.\"\"\"\n",
    "\n",
    "print(\"üéØ FINAL DEMONSTRATION: Complex Multi-Aspect Query\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query: {complex_query}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process with maximum context for comprehensive analysis\n",
    "final_result = advanced_rag_system.process_query_with_comparison(\n",
    "    complex_query, \n",
    "    retrieve_k=len(documents), \n",
    "    rerank_k=6\n",
    ")\n",
    "\n",
    "display_comprehensive_results(final_result)\n",
    "\n",
    "print(\"üéâ CROSS-ENCODER RERANKING DEMO COMPLETE!\")\n",
    "print(\"\\nüìã What we've demonstrated:\")\n",
    "print(\"‚úÖ Cross-encoder vs bi-encoder reranking comparison\")\n",
    "print(\"‚úÖ Superior relevance detection through joint query-document processing\")\n",
    "print(\"‚úÖ Performance vs quality trade-off analysis\")\n",
    "print(\"‚úÖ Real-world computational cost considerations\")\n",
    "print(\"‚úÖ Integration with state-of-the-art language models\")\n",
    "print(\"‚úÖ Comprehensive evaluation metrics and visualizations\")\n",
    "\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(\"‚Ä¢ Cross-encoders provide more nuanced relevance scoring\")\n",
    "print(\"‚Ä¢ Higher computational cost but better quality for critical applications\")\n",
    "print(\"‚Ä¢ Significant ranking differences show value of cross-encoder approach\")\n",
    "print(\"‚Ä¢ Best suited for high-value queries where accuracy is paramount\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for production considerations:\")\n",
    "print(\"‚Ä¢ Use cross-encoders for final reranking of top candidates\")\n",
    "print(\"‚Ä¢ Implement caching for repeated queries\")\n",
    "print(\"‚Ä¢ Consider hybrid approaches based on query complexity\")\n",
    "print(\"‚Ä¢ Monitor user satisfaction to validate ranking improvements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration with a complex query\n",
    "complex_query = \"Compare the exploration strategies for Mars and Europa, focusing on the search for life\"\n",
    "\n",
    "print(\"üéØ FINAL DEMONSTRATION: Complex Query\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_result = rag_system.process_query(complex_query, retrieve_k=8, rerank_k=4)\n",
    "display_results(final_result)\n",
    "\n",
    "print(\"üéâ Semantic Reranking RAG System Demo Complete!\")\n",
    "print(\"\\nThis notebook demonstrated:\")\n",
    "print(\"‚úÖ Document embedding and indexing\")\n",
    "print(\"‚úÖ Semantic similarity-based reranking\")\n",
    "print(\"‚úÖ Integration with Gemini for explanations\")\n",
    "print(\"‚úÖ Performance analysis and comparison\")\n",
    "print(\"‚úÖ Complete RAG pipeline implementation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
